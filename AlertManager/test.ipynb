{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'David'],\n",
    "    'age': [25, 17, 65, 45, 18],\n",
    "    'salary': [50000, 60000, 200000, 75000, 75000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235d61cb9e8756ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "class LocalValidator:\n",
    "\n",
    "    def __init__(self, store=False, history=False, united=True, path=\"./validation_logs\", file_type=\"pkl\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            store (bool): Whether to store validation results.\n",
    "            history (bool): Whether to store logs with historical data.\n",
    "            united (bool): Whether to store all validations in one file or separately.\n",
    "            path (str): Directory path where logs will be stored.\n",
    "            file_type (str): The file format for storing validation results. Options are 'csv', 'xlsx', 'pkl', 'txt'.\n",
    "\n",
    "        Raises:\n",
    "            TypeError: If any of the input arguments are not of the expected type.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # Initialize attributes based on user input\n",
    "        self.store = store  # Determines whether to store validation results\n",
    "        self.united = united  # Determines whether to store all validations in one file\n",
    "        self.history = history  # Determines whether to store logs with historical data\n",
    "        self.file_type = file_type.lower()  # File type for storing validation results\n",
    "\n",
    "        # Set the path for storing logs, including daily subdirectories if history is True\n",
    "        if history:\n",
    "            self.path = os.path.join(path, f\"{datetime.now().strftime('%Y-%m-%d')}\")\n",
    "        else:\n",
    "            self.path = path\n",
    "\n",
    "        # Initialize an empty DataFrame for storing all validation results if united is True\n",
    "        self.all_validations_df = pd.DataFrame()\n",
    "\n",
    "        # Validate the types of the input arguments\n",
    "        if not isinstance(store, bool):\n",
    "            raise TypeError(\"The 'store' argument must be a boolean.\")\n",
    "        if not isinstance(united, bool):\n",
    "            raise TypeError(\"The 'united' argument must be a boolean.\")\n",
    "        if not isinstance(history, bool):\n",
    "            raise TypeError(\"The 'history' argument must be a boolean.\")\n",
    "        if not isinstance(file_type, str):\n",
    "            raise TypeError(\"The 'file_type' argument must be a string.\")\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "            \n",
    "            \n",
    "\n",
    "    def range_check(self, *, column: str, borders: list, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to validate that the values in a specified column fall within given ranges.\n",
    "    \n",
    "        Args:\n",
    "            column (str): The column in the DataFrame to be validated.\n",
    "            borders (list): A list of tuples, each containing two numeric values representing the lower and upper bounds.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "    \n",
    "        Returns:\n",
    "            function: A wrapped function with the validation applied.\n",
    "    \n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Validate input types\n",
    "        if not isinstance(column, str):\n",
    "            raise TypeError(\"The 'column' argument must be a string.\")\n",
    "        if not isinstance(borders, list) or not all(isinstance(i, tuple) and len(i) == 2 for i in borders):\n",
    "            raise TypeError(\"The 'borders' argument must be a list of tuples with two numeric values.\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "    \n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Check if the specified column exists in the DataFrame\n",
    "                if column not in df.columns:\n",
    "                    raise TypeError(f\"Error: Column '{column}' not found in DataFrame.\")\n",
    "    \n",
    "                # Initialize a boolean Series to track whether values are within any of the specified ranges\n",
    "                in_range_mask = pd.Series([False] * len(df))\n",
    "    \n",
    "                # Iterate over the list of borders and update the mask for values within the range\n",
    "                for bottom, top in borders:\n",
    "                    in_range_mask |= df[column].between(bottom, top)\n",
    "    \n",
    "                # Identify rows where values are out of bounds\n",
    "                out_of_bounds = df.loc[~in_range_mask].copy()\n",
    "    \n",
    "                # Save the out-of-bounds rows if any exist and storing is enabled\n",
    "                if not out_of_bounds.empty and self.store:\n",
    "                    self.save(out_of_bounds, name)\n",
    "    \n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "    \n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "\n",
    "\n",
    "    def value_check(self, *, column: str, allowed: list = None, not_allowed: list = None, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to validate that the values in a specified column are either allowed or not allowed.\n",
    "    \n",
    "        Args:\n",
    "            column (str): The column in the DataFrame to be validated.\n",
    "            allowed (list, optional): A list of allowed values for the column.\n",
    "            not_allowed (list, optional): A list of not allowed values for the column.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "    \n",
    "        Returns:\n",
    "            function: A wrapped function with the validation applied.\n",
    "    \n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "        \"\"\"\n",
    "    \n",
    "        # Validate input types\n",
    "        if not isinstance(column, str):\n",
    "            raise TypeError(\"The 'column' argument must be a string.\")\n",
    "        if allowed is not None and not isinstance(allowed, list):\n",
    "            raise TypeError(\"The 'allowed' argument must be a list.\")\n",
    "        if not_allowed is not None and not isinstance(not_allowed, list):\n",
    "            raise TypeError(\"The 'not_allowed' argument must be a list.\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "    \n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Check if the specified column exists in the DataFrame\n",
    "                if column not in df.columns:\n",
    "                    raise TypeError(f\"Error: Column '{column}' not found in DataFrame.\")\n",
    "    \n",
    "                # Initialize an empty DataFrame to store invalid rows\n",
    "                invalid_rows = pd.DataFrame()\n",
    "    \n",
    "                # Validate against the allowed list, if provided\n",
    "                if allowed is not None:\n",
    "                    invalid_rows_allowed = df[~df[column].isin(allowed)]\n",
    "                    invalid_rows = pd.concat([invalid_rows, invalid_rows_allowed])\n",
    "    \n",
    "                # Validate against the not allowed list, if provided\n",
    "                if not_allowed is not None:\n",
    "                    invalid_rows_not_allowed = df[df[column].isin(not_allowed)]\n",
    "                    invalid_rows = pd.concat([invalid_rows, invalid_rows_not_allowed])\n",
    "    \n",
    "                # Save the invalid rows if any exist and storing is enabled\n",
    "                if not invalid_rows.empty and self.store:\n",
    "                    self.save(invalid_rows, name)\n",
    "    \n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "    \n",
    "            return wrapper\n",
    "        return decorator\n",
    "\n",
    "\n",
    "\n",
    "    def custom_check(self, *, custom_logic, name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Decorator to apply custom validation logic on a DataFrame.\n",
    "    \n",
    "        Args:\n",
    "            custom_logic (str or callable): The custom logic for validation, can be a query string or a function.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "    \n",
    "        Returns:\n",
    "            function: A wrapped function with the custom validation applied.\n",
    "    \n",
    "        Raises:\n",
    "            TypeError: If input arguments are not of the expected type.\n",
    "            ValueError: If the custom logic string or function fails to execute.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Validate input types\n",
    "        if not (isinstance(custom_logic, str) or callable(custom_logic)):\n",
    "            raise TypeError(\"The 'custom_logic' argument must be a string or a callable (function).\")\n",
    "        if not isinstance(name, str):\n",
    "            raise TypeError(\"The 'name' argument must be a string.\")\n",
    "    \n",
    "        def decorator(func):\n",
    "            def wrapper(df, *args, **kwargs_func):\n",
    "                # Apply custom logic if it's a string (query)\n",
    "                if isinstance(custom_logic, str):\n",
    "                    try:\n",
    "                        invalid_rows = df.query(custom_logic)\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error in custom logic: {str(e)}\")\n",
    "                \n",
    "                # Apply custom logic if it's a callable (function)\n",
    "                elif callable(custom_logic):\n",
    "                    try:\n",
    "                        invalid_rows = custom_logic(df)\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Error in custom function: {str(e)}\")\n",
    "    \n",
    "                    # Convert Series result to DataFrame for consistency\n",
    "                    if isinstance(invalid_rows, pd.Series):\n",
    "                        invalid_rows = df.loc[invalid_rows].copy()\n",
    "                    elif not isinstance(invalid_rows, pd.DataFrame):\n",
    "                        raise TypeError(\"The custom function must return a pandas Series or DataFrame.\")\n",
    "    \n",
    "                # Save the invalid rows if any exist and storing is enabled\n",
    "                if not invalid_rows.empty and self.store:\n",
    "                    self.save(invalid_rows, name)\n",
    "    \n",
    "                # Execute the wrapped function with the original arguments\n",
    "                return func(df, *args, **kwargs_func)\n",
    "            \n",
    "            return wrapper\n",
    "        \n",
    "        return decorator\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, outliers, name):\n",
    "        \"\"\"\n",
    "        Saves the outliers to a file based on the validator settings.\n",
    "    \n",
    "        Args:\n",
    "            outliers (pd.DataFrame): DataFrame containing the outliers.\n",
    "            name (str): The name of the validation for logging purposes.\n",
    "        \"\"\"\n",
    "        # Create a copy of the outliers DataFrame to avoid modifying the original\n",
    "        outliers = outliers.copy()\n",
    "    \n",
    "        # Add a new column to track the name of the validation that generated the outliers\n",
    "        outliers[\"Validation Name\"] = name\n",
    "    \n",
    "        # If united is True, concatenate the outliers with the existing DataFrame of all validations\n",
    "        if self.united:\n",
    "            self.all_validations_df = pd.concat([self.all_validations_df, outliers], ignore_index=True)\n",
    "            # Save the combined DataFrame to a file named 'log' in the specified path\n",
    "            self.save_file(self.all_validations_df, os.path.join(self.path, \"log\"))\n",
    "        else:\n",
    "            # Save the outliers DataFrame to a file named after the validation name\n",
    "            self.save_file(outliers, os.path.join(self.path, f\"{name}\"))\n",
    "\n",
    "\n",
    "\n",
    "    def save_file(self, df, file_name):\n",
    "        \"\"\"\n",
    "        Saves a DataFrame to a file in the specified format.\n",
    "    \n",
    "        Args:\n",
    "            df (pd.DataFrame): The DataFrame to save.\n",
    "            file_name (str): The path and base name of the file.\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If the specified file type is not supported.\n",
    "        \"\"\"\n",
    "        # Check the file type and save the DataFrame accordingly\n",
    "        if self.file_type == \"csv\":\n",
    "            df.to_csv(f\"{file_name}.csv\", index=False, encoding='utf-8')\n",
    "        elif self.file_type == \"xlsx\":\n",
    "            df.to_excel(f\"{file_name}.xlsx\", index=False)\n",
    "        elif self.file_type == \"pkl\":\n",
    "            df.to_pickle(f\"{file_name}.pkl\")\n",
    "        elif self.file_type == \"txt\":\n",
    "            with open(f\"{file_name}.txt\", \"w\") as log:\n",
    "                df.to_string(log)\n",
    "                log.write(\"\\n\")\n",
    "        else:\n",
    "            # Raise an error if the file type is not supported\n",
    "            raise ValueError(\"Unsupported file type. Supported types are: 'csv', 'xlsx', 'pkl', 'txt'\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2d5aed01cd1e7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Example usage of the LocalValidator class.\n",
    "validator = LocalValidator(store=True, history=True, united=True, file_type=\"csv\")\n",
    "\n",
    "# Define custom validation logic as a string\n",
    "custom_logic_str = \"salary == 200000 | name == 'Alice'\"\n",
    "\n",
    "# Define custom validation logic as a function\n",
    "def custom_logic_func(dfss):\n",
    "    return dfss['salary'] == 200000\n",
    "\n",
    "@validator.range_check(column='age', borders=[(18, 30), (50, np.inf)], name=\"Val1\")\n",
    "@validator.value_check(column='name', allowed=['Alice', 'Bob', 'Charlie'], name=\"Val2\")\n",
    "@validator.value_check(column='name', not_allowed=['David'], name=\"Val3\")\n",
    "@validator.custom_check(custom_logic=custom_logic_str, name=\"CustomCheckStr\")\n",
    "@validator.custom_check(custom_logic=custom_logic_func, name=\"CustomCheckFunc\")\n",
    "def process_data_1(df):\n",
    "    print(\"Processing data for validation 1...\")\n",
    "\n",
    "# Apply the decorators and process a DataFrame (df must be defined earlier in the code).\n",
    "process_data_1(df)\n"
   ],
   "id": "26a2a9c3c8f26332",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# დავამატოთ დათაფრეიმში აუთლეიერების პონის ფუნქციონალი, რომელიც გადაცემული სვეტის ან სვეტების მიხედვით იპოვის აუთლეიერს და გამოუტანს იუზერს, თუ სვეტს არ დაუკონკრეტებ მაშინ დათაფრეიმის თითოეულ სვეტში მოგიძებნის აუთლეიერებს. \n",
    "# ზემოთ დაწერილი ფუნქციონალი უნდა დაიწეროს MS SQL, PostgreSQL, MySQL"
   ],
   "id": "fb2ae68989d45f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "\n",
    "def z_score(df):\n",
    "    mean = np.mean(df)\n",
    "    std_dev = np.std(df)\n",
    "    z_scores = np.abs((df - mean) / std_dev)\n",
    "    \n",
    "    threshold = 3  # Common threshold\n",
    "    outliers_zscore = df[z_scores > threshold]\n",
    "    \n",
    "    return outliers_zscore\n",
    "\n",
    "def IQR(df):\n",
    "    Q1 = np.percentile(df, 25)\n",
    "    Q3 = np.percentile(df, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_iqr = df[(df < lower_bound) | (df > upper_bound)]\n",
    "    \n",
    "    return  outliers_iqr\n",
    "\n",
    "\n",
    "def isolate(df):\n",
    "    clf = IsolationForest(contamination=0.01)\n",
    "    outliers_ml = clf.fit_predict(df)\n",
    "    \n",
    "    outliers = df[outliers_ml == -1]\n",
    "    \n",
    "    return  outliers\n",
    "\n",
    "\n",
    "def localot(df):\n",
    "    # Apply Local Outlier Factor (LOF)\n",
    "    clf = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "    outliers_lof = clf.fit_predict(df)\n",
    "    \n",
    "    # LOF labels outliers as -1\n",
    "    outliers = df[outliers_lof == -1]\n",
    "    \n",
    "    return  outliers\n",
    "    \n",
    "    \n"
   ],
   "id": "ddfe54c603cb428c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "def freq(df):\n",
    "    frequency_counts = pd.Series(df).value_counts()\n",
    "    \n",
    "    low_frequency_threshold = 5\n",
    "    outliers_frequency = frequency_counts[frequency_counts < low_frequency_threshold].index.tolist()\n",
    "    \n",
    "    return outliers_frequency\n",
    "\n",
    "\n",
    "\n",
    "def dbs(df):\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "    outliers_dbscan = dbscan.fit_predict(df)\n",
    "    \n",
    "    outliers = df[outliers_dbscan == -1]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "\n",
    "\n",
    "def strbas(df):\n",
    "    # Convert the strings to a matrix of TF-IDF features\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(df)\n",
    "    \n",
    "    # Fit DBSCAN\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(X)\n",
    "    \n",
    "    # Identify outliers (outliers are labeled as -1)\n",
    "    outliers_dbscan = np.array(df)[labels == -1]\n",
    "    \n",
    "    return outliers_dbscan\n"
   ],
   "id": "90d4a2e3798fcf6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c363bec637abfa5c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
